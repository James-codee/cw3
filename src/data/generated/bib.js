const generatedBibEntries = {
    "Afroogh2024TrustAI": {
        "abstract": "The increasing use of artificial intelligence (AI) systems in our daily lives through various applications, services, and products highlights the significance of trust and distrust in AI from a user perspective. AI-driven systems have significantly diffused into various aspects of our lives, serving as beneficial \u201ctools\u201d used by human agents. These systems are also evolving to act as co-assistants or semi-agents in specific domains, potentially influencing human thought, decision-making, and agency. Trust and distrust in AI serve as regulators and could significantly control the level of this diffusion, as trust can increase, and distrust may reduce the rate of adoption of AI. Recently, a variety of studies focused on the different dimensions of trust and distrust in AI and its relevant considerations. In this systematic literature review, after conceptualizing trust in the current AI literature, we will investigate trust in different types of human\u2013machine interaction and its impact on technology acceptance in different domains. Additionally, we propose a taxonomy of technical (i.e., safety, accuracy, robustness) and non-technical axiological (i.e., ethical, legal, and mixed) trustworthiness metrics, along with some trustworthy measurements. Moreover, we examine major trust-breakers in AI (e.g., autonomy and dignity threats) and trustmakers; and propose some future directions and probable solutions for the transition to a trustworthy AI.",
        "author": "Saleh Afroogh and Ali Akbari and Emmie Malone and Mohammadali Kargar and Hananeh Alambeigi",
        "doi": "10.1057/s41599-024-04044-8",
        "journal": "Humanities and Social Sciences Communications",
        "keywords": "type: review, trust in AI, challenges, future directions",
        "number": "1",
        "pages": "1--15",
        "publisher": "Springer Nature",
        "series": "AI Trust Futures",
        "title": "Trust in AI: Progress, Challenges, and Future Directions",
        "type": "article",
        "url": "https://doi.org/10.1057/s41599-024-04044-8",
        "volume": "11",
        "year": "2024"
    },
    "Barredo2020XAI": {
        "abstract": "In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.",
        "author": "Alejandro Barredo Arrieta and Natalia D\u00edaz-Rodr\u00edguez and Javier Del Ser and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador Garc\u00eda and Sergio Gil-L\u00f3pez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera",
        "doi": "10.1016/j.inffus.2019.12.012",
        "journal": "Information Fusion",
        "keywords": "type: taxonomy, explainable AI, taxonomy, responsible AI",
        "number": "",
        "pages": "82--115",
        "publisher": "Elsevier",
        "series": "XAI Foundations",
        "title": "Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI",
        "type": "article",
        "url": "https://doi.org/10.1016/j.inffus.2019.12.012",
        "volume": "58",
        "year": "2020"
    },
    "Gerlich2023PerceptionsAI": {
        "abstract": "In this comprehensive study, insights from 1389 scholars across the US, UK, Germany, and Switzerland shed light on the multifaceted perceptions of artificial intelligence (AI). AI\u2019s burgeoning integration into everyday life promises enhanced efficiency and innovation. The Trustworthy AI principles by the European Commission, emphasising data safeguarding, security, and judicious governance, serve as the linchpin for AI\u2019s widespread acceptance. A correlation emerged between societal interpretations of AI\u2019s impact and elements like trustworthiness, associated risks, and usage/acceptance. Those discerning AI\u2019s threats often view its prospective outcomes pessimistically, while proponents recognise its transformative potential. These inclinations resonate with trust and AI\u2019s perceived singularity. Consequently, factors such as trust, application breadth, and perceived vulnerabilities shape public consensus, depicting AI as humanity\u2019s boon or bane. The study also accentuates the public\u2019s divergent views on AI\u2019s evolution, underlining the malleability of opinions amidst polarising narratives.",
        "author": "Michael Gerlich",
        "doi": "10.3390/socsci12090502",
        "journal": "Social Sciences",
        "keywords": "type: empirical study, AI perception, user acceptance, multidimensional study",
        "number": "9",
        "pages": "502",
        "publisher": "MDPI",
        "series": "Public Perception AI",
        "title": "Perceptions and Acceptance of Artificial Intelligence: A Multi-Dimensional Study",
        "type": "article",
        "url": "https://doi.org/10.3390/socsci12090502",
        "volume": "12",
        "year": "2023"
    },
    "Glikson2020TrustAI": {
        "abstract": "Artificial intelligence (AI) characterizes a new generation of technologies capable of interacting with the environment and aiming to simulate human intelligence. The success of integrating AI into organizations critically depends on workers\u2019 trust in AI technology. This review explains how AI differs from other technologies and presents the existing empirical research on the determinants of human \u201ctrust\u201d in AI, conducted in multiple disciplines over the last 20 years. Based on the reviewed literature, we identify the form of AI representation (robot, virtual, and embedded) and its level of machine intelligence (i.e., its capabilities) as important antecedents to the development of trust and propose a framework that addresses the elements that shape users\u2019 cognitive and emotional trust. Our review reveals the important role of AI\u2019s tangibility, transparency, reliability, and immediacy behaviors in developing cognitive trust, and the role of AI\u2019s anthropomorphism specifically for emotional trust. We also note several limitations in the current evidence base, such as the diversity of trust measures and overreliance on short-term, small sample, and experimental studies, where the development of trust is likely to be different than in longer-term, higher stakes field environments. Based on our review, we suggest the most promising paths for future research.",
        "author": "Ella Glikson and Anita W. Woolley",
        "doi": "10.5465/annals.2018.0057",
        "journal": "Academy of Management Annals",
        "keywords": "type: review, trust, artificial intelligence, empirical research",
        "number": "2",
        "pages": "627--660",
        "publisher": "Academy of Management",
        "series": "AI Trust Analysis",
        "title": "Human Trust in Artificial Intelligence: Review of Empirical Research",
        "type": "article",
        "url": "https://doi.org/10.5465/annals.2018.0057",
        "volume": "14",
        "year": "2020"
    },
    "Henrique2024Trust": {
        "abstract": "Artificial Intelligence (AI) is present in various modern systems, but it is still subject to acceptance in many fields. Medical diagnosis, autonomous driving cars, recommender systems and robotics are examples of areas in which some humans distrust AI technology, which ultimately leads to low acceptance rates. Conversely, those same applications can have humans who over rely on AI, acting as recommended by the systems with no criticism regarding the risks of a wrong decision. Therefore, there is an optimal balance with respect to trust in AI, achieved by calibration of expectations and capabilities. In this context, the literature about factors influencing trust in AI and its calibration is scattered among research fields, with no objective summaries of the overall evolution of the theme. In order to close this gap, this paper contributes a literature review of the most influential papers on the subject of trust in AI, selected by quantitative methods. It also proposes a Main Path Analysis of the literature, highlighting how the theme has evolved over the years. As results, researchers will find an overview on trust in AI based on the most important papers objectively selected and also tendencies and opportunities for future research.",
        "author": "Bruno Miranda Henrique and Eugene Santos",
        "doi": "10.1016/j.chbah.2024.100043",
        "journal": "Computers in Human Behavior: Artificial Humans",
        "keywords": "type: literature review, trust in AI, literature review, main path analysis",
        "number": "1",
        "pages": "100043",
        "publisher": "Elsevier BV",
        "series": "AI Trust Reviews",
        "title": "Trust in Artificial Intelligence: Literature Review and Main Path Analysis",
        "type": "article",
        "url": "https://doi.org/10.1016/j.chbah.2024.100043",
        "volume": "2",
        "year": "2024"
    },
    "Holzinger2020Explainability": {
        "abstract": "Artificial intelligence and algorithmic decision-making processes are increasingly criticized for their black-box nature. Explainable AI approaches to trace human-interpretable decision processes from algorithms have been explored. Yet, little is known about algorithmic explainability from a human factors\u2019 perspective. From the perspective of user interpretability and understandability, this study examines the effect of explainability in AI on user trust and attitudes toward AI. It conceptualizes causability as an antecedent of explainability and as a key cue of an algorithm and examines them in relation to trust by testing how they affect user perceived performance of AI-driven services. The results show the dual roles of causability and explainability in terms of its underlying links to trust and subsequent user behaviors. Explanations of why certain news articles are recommended generate users trust whereas causability of to what extent they can understand the explanations affords users emotional confidence. Causability lends the justification for what and how should be explained as it determines the relative importance of the properties of explainability. The results have implications for the inclusion of causability and explanatory cues in AI systems, which help to increase trust and help users to assess the quality of explanations. Causable explainable AI will help people understand the decision-making process of AI algorithms by bringing transparency and accountability into AI systems.",
        "author": "Donghee Shin",
        "doi": "10.1016/j.ijhcs.2020.102551",
        "journal": "International Journal of Human-Computer Studies",
        "keywords": "type: experimental, explainability, causability, trust, AI acceptance",
        "number": "",
        "pages": "102551",
        "publisher": "Elsevier",
        "series": "Explainability Effects",
        "title": "The Effects of Explainability and Causability on Perception, Trust, and Acceptance of AI",
        "type": "article",
        "url": "https://doi.org/10.1016/j.ijhcs.2020.102551",
        "volume": "146",
        "year": "2020"
    },
    "Hulsen2023XAI": {
        "abstract": "Artificial Intelligence (AI) describes computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. Examples of AI techniques are machine learning, neural networks, and deep learning. AI can be applied in many different areas, such as econometrics, biometry, e-commerce, and the automotive industry. In recent years, AI has found its way into healthcare as well, helping doctors make better decisions (\u201cclinical decision support\u201d), localizing tumors in magnetic resonance images, reading and analyzing reports written by radiologists and pathologists, and much more. However, AI has one big risk: it can be perceived as a \u201cblack box\u201d, limiting trust in its reliability, which is a very big issue in an area in which a decision can mean life or death. As a result, the term Explainable Artificial Intelligence (XAI) has been gaining momentum. XAI tries to ensure that AI algorithms (and the resulting decisions) can be understood by humans. In this narrative review, we will have a look at some central concepts in XAI, describe several challenges around XAI in healthcare, and discuss whether it can really help healthcare to advance, for example, by increasing understanding and trust. Finally, alternatives to increase trust in AI are discussed, as well as future research possibilities in the area of XAI.",
        "author": "Tim Hulsen",
        "doi": "10.3390/ai4030034",
        "journal": "AI",
        "keywords": "type: review, explainable AI, healthcare, challenges",
        "number": "3",
        "pages": "652--666",
        "publisher": "MDPI",
        "series": "XAI Healthcare",
        "title": "Explainable Artificial Intelligence (XAI): Concepts and Challenges in Healthcare",
        "type": "article",
        "url": "https://doi.org/10.3390/ai4030034",
        "volume": "4",
        "year": "2023"
    },
    "Rong2024HumanXAI": {
        "abstract": "Explainable AI (XAI) is widely viewed as a sine qua non for ever-expanding AI research. A better understanding of the needs of XAI users, as well as human-centered evaluations of explainable models are both a necessity and a challenge. In this paper, we explore how human-computer interaction (HCI) and AI researchers conduct user studies in XAI applications based on a systematic literature review. After identifying and thoroughly analyzing 97 core papers with human-based XAI evaluations over the past five years, we categorize them along the measured characteristics of explanatory methods, namely trust, understanding, usability, and human-AI collaboration performance. Our research shows that XAI is spreading more rapidly in certain application domains, such as recommender systems than in others, but that user evaluations are still rather sparse and incorporate hardly any insights from cognitive or social sciences. Based on a comprehensive discussion of best practices, i.e., common models, design choices, and measures in user studies, we propose practical guidelines on designing and conducting user studies for XAI researchers and practitioners. Lastly, this survey also highlights several open research directions, particularly linking psychological science and human-centered XAI.",
        "author": "Yao Rong and Tobias Leemann and Thai-Trang Nguyen and Lisa Fiedler and Peizhu Qian and Vaibhav V. Unhelkar and Tina Seidel and Gjergji Kasneci and Enkelejda Kasneci",
        "doi": "10.1109/TPAMI.2023.3331846",
        "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "keywords": "type: survey, human-centered AI, explainable AI, user studies, model explanations",
        "number": "4",
        "pages": "2104--2122",
        "publisher": "IEEE",
        "series": "Human-centered AI",
        "title": "Towards Human-centered Explainable AI: A Survey of User Studies for Model Explanations",
        "type": "article",
        "url": "https://doi.org/10.1109/TPAMI.2023.3331846",
        "volume": "46",
        "year": "2024"
    },
    "Schmid2024UserStudy": {
        "abstract": "Choosing a career and educational path is a challenging decision for young people. Career planning conversational agents (CAs) can assist by identifying suitable occupations and educational paths. Trustworthiness is an important dimension for the acceptance of a career planning CA and is influenced by several factors. We conducted a user study with n",
        "author": "Thorsten Zylowski and Nathalia Sautchuk-Patr\u00edcio and Wladimir Hettmann",
        "doi": "10.1145/3702163.3702409",
        "journal": "Proceedings of the 2024 ACM Conference on Human Factors in Computing Systems",
        "keywords": "type: user study, trustworthiness, usability, explainability, conversational agents",
        "number": "",
        "pages": "1--10",
        "publisher": "ACM",
        "series": "Conversational Agent UX",
        "title": "User Study on the Trustworthiness, Usability and Explainability of Intent-based and Large Language Model-based Career Planning Conversational Agents",
        "type": "article",
        "url": "https://doi.org/10.1145/3702163.3702409",
        "volume": "",
        "year": "2024"
    },
    "Visser2025TrustSurvey": {
        "abstract": "A current concern in the field of Artificial Intelligence (AI) is to ensure the trustworthiness of AI systems. The development of explainability methods is one prominent way to address this, which has often resulted in the assumption that the use of explainability will lead to an increase in the trust of users and wider society. However, the dynamics between explainability and trust are not well established and empirical investigations of their relation remain mixed or inconclusive. In this paper we provide a detailed description of the concepts of user trust and distrust in AI and their relation to appropriate reliance. For that we draw from the fields of machine learning, human\u2013computer interaction, and the social sciences. Based on these insights, we have created a focused study of empirical literature of existing empirical studies that investigate the effects of AI systems and XAI methods on user (dis)trust, in order to substantiate our conceptualization of trust, distrust, and reliance. With respect to our conceptual understanding we identify gaps in existing empirical work. With clarifying the concepts and summarizing the empirical studies, we aim to provide researchers, who examine user trust in AI, with an improved starting point for developing user studies to measure and evaluate the user\u2019s attitude towards and reliance on AI systems.",
        "author": "Roel Visser and Tobias M. Peters and Ingrid Scharlau and Barbara Hammer",
        "doi": "10.1016/j.cogsys.2025.101357",
        "journal": "Cognitive Systems Research",
        "keywords": "type: survey, trust, distrust, explainable AI, user trust evaluation",
        "number": "1",
        "pages": "101357",
        "publisher": "Elsevier",
        "series": "Empirical XAI Studies",
        "title": "Trust, Distrust, and Appropriate Reliance in (X)AI: A Survey of Empirical Evaluation of User Trust",
        "type": "article",
        "url": "https://doi.org/10.1016/j.cogsys.2025.101357",
        "volume": "80",
        "year": "2025"
    }
};