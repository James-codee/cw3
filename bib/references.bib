@article{Henrique2024Trust,
  abstract = {Artificial Intelligence (AI) is present in various modern systems, but it is still subject to acceptance in many fields. Medical diagnosis, autonomous driving cars, recommender systems and robotics are examples of areas in which some humans distrust AI technology, which ultimately leads to low acceptance rates. Conversely, those same applications can have humans who over rely on AI, acting as recommended by the systems with no criticism regarding the risks of a wrong decision. Therefore, there is an optimal balance with respect to trust in AI, achieved by calibration of expectations and capabilities. In this context, the literature about factors influencing trust in AI and its calibration is scattered among research fields, with no objective summaries of the overall evolution of the theme. In order to close this gap, this paper contributes a literature review of the most influential papers on the subject of trust in AI, selected by quantitative methods. It also proposes a Main Path Analysis of the literature, highlighting how the theme has evolved over the years. As results, researchers will find an overview on trust in AI based on the most important papers objectively selected and also tendencies and opportunities for future research.},
  author = {Bruno Miranda Henrique and Eugene Santos},
  title = {Trust in Artificial Intelligence: Literature Review and Main Path Analysis},
  journal = {Computers in Human Behavior: Artificial Humans},
  year = {2024},
  volume = {2},
  number = {1},
  pages = {100043},
  publisher = {Elsevier BV},
  doi = {10.1016/j.chbah.2024.100043},
  url = {https://doi.org/10.1016/j.chbah.2024.100043},
  keywords = {type: literature review, trust in AI, literature review, main path analysis}
  series = {AI Trust Reviews}
}

@article{Hulsen2023XAI,
  abstract = {Artificial Intelligence (AI) describes computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. Examples of AI techniques are machine learning, neural networks, and deep learning. AI can be applied in many different areas, such as econometrics, biometry, e-commerce, and the automotive industry. In recent years, AI has found its way into healthcare as well, helping doctors make better decisions (“clinical decision support”), localizing tumors in magnetic resonance images, reading and analyzing reports written by radiologists and pathologists, and much more. However, AI has one big risk: it can be perceived as a “black box”, limiting trust in its reliability, which is a very big issue in an area in which a decision can mean life or death. As a result, the term Explainable Artificial Intelligence (XAI) has been gaining momentum. XAI tries to ensure that AI algorithms (and the resulting decisions) can be understood by humans. In this narrative review, we will have a look at some central concepts in XAI, describe several challenges around XAI in healthcare, and discuss whether it can really help healthcare to advance, for example, by increasing understanding and trust. Finally, alternatives to increase trust in AI are discussed, as well as future research possibilities in the area of XAI.},
  author = {Tim Hulsen},
  title = {Explainable Artificial Intelligence (XAI): Concepts and Challenges in Healthcare},
  journal = {AI},
  year = {2023},
  volume = {4},
  number = {3},
  pages = {652--666},
  publisher = {MDPI},
  doi = {10.3390/ai4030034},
  url = {https://doi.org/10.3390/ai4030034},
  keywords = {type: review, explainable AI, healthcare, challenges}
  series = {XAI Healthcare}
}

@article{Rong2024HumanXAI,
  abstract = {Explainable AI (XAI) is widely viewed as a sine qua non for ever-expanding AI research. A better understanding of the needs of XAI users, as well as human-centered evaluations of explainable models are both a necessity and a challenge. In this paper, we explore how human-computer interaction (HCI) and AI researchers conduct user studies in XAI applications based on a systematic literature review. After identifying and thoroughly analyzing 97 core papers with human-based XAI evaluations over the past five years, we categorize them along the measured characteristics of explanatory methods, namely trust, understanding, usability, and human-AI collaboration performance. Our research shows that XAI is spreading more rapidly in certain application domains, such as recommender systems than in others, but that user evaluations are still rather sparse and incorporate hardly any insights from cognitive or social sciences. Based on a comprehensive discussion of best practices, i.e., common models, design choices, and measures in user studies, we propose practical guidelines on designing and conducting user studies for XAI researchers and practitioners. Lastly, this survey also highlights several open research directions, particularly linking psychological science and human-centered XAI.},
  author = {Yao Rong and Tobias Leemann and Thai-Trang Nguyen and Lisa Fiedler and Peizhu Qian and Vaibhav V. Unhelkar and Tina Seidel and Gjergji Kasneci and Enkelejda Kasneci},
  title = {Towards Human-centered Explainable AI: A Survey of User Studies for Model Explanations},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2024},
  volume = {46},
  number = {4},
  pages = {2104--2122},
  publisher = {IEEE},
  doi = {10.1109/TPAMI.2023.3331846},
  url = {https://doi.org/10.1109/TPAMI.2023.3331846},
  keywords = {type: survey, human-centered AI, explainable AI, user studies, model explanations}
  series = {Human-centered AI}
}

@article{Visser2025TrustSurvey,
  abstract = {A current concern in the field of Artificial Intelligence (AI) is to ensure the trustworthiness of AI systems. The development of explainability methods is one prominent way to address this, which has often resulted in the assumption that the use of explainability will lead to an increase in the trust of users and wider society. However, the dynamics between explainability and trust are not well established and empirical investigations of their relation remain mixed or inconclusive.
In this paper we provide a detailed description of the concepts of user trust and distrust in AI and their relation to appropriate reliance. For that we draw from the fields of machine learning, human–computer interaction, and the social sciences. Based on these insights, we have created a focused study of empirical literature of existing empirical studies that investigate the effects of AI systems and XAI methods on user (dis)trust, in order to substantiate our conceptualization of trust, distrust, and reliance. With respect to our conceptual understanding we identify gaps in existing empirical work. With clarifying the concepts and summarizing the empirical studies, we aim to provide researchers, who examine user trust in AI, with an improved starting point for developing user studies to measure and evaluate the user’s attitude towards and reliance on AI systems.},
  author = {Roel Visser and Tobias M. Peters and Ingrid Scharlau and Barbara Hammer},
  title = {Trust, Distrust, and Appropriate Reliance in (X)AI: A Survey of Empirical Evaluation of User Trust},
  journal = {Cognitive Systems Research},
  year = {2025},
  volume = {80},
  number = {1},
  pages = {101357},
  publisher = {Elsevier},
  doi = {10.1016/j.cogsys.2025.101357},
  url = {https://doi.org/10.1016/j.cogsys.2025.101357},
  keywords = {type: survey, trust, distrust, explainable AI, user trust evaluation}
  series = {Empirical XAI Studies}
}

@article{Barredo2020XAI,
  abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
  author = {Alejandro Barredo Arrieta and Natalia Díaz-Rodríguez and Javier Del Ser and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador García and Sergio Gil-López and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera},
  title = {Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI},
  journal = {Information Fusion},
  year = {2020},
  volume = {58},
  number = {},
  pages = {82--115},
  publisher = {Elsevier},
  doi = {10.1016/j.inffus.2019.12.012},
  url = {https://doi.org/10.1016/j.inffus.2019.12.012},
  keywords = {type: taxonomy, explainable AI, taxonomy, responsible AI}
  series = {XAI Foundations}
}

@article{Glikson2020TrustAI,
  abstract = {Artificial intelligence (AI) characterizes a new generation of technologies capable of interacting with the environment and aiming to simulate human intelligence. The success of integrating AI into organizations critically depends on workers’ trust in AI technology. This review explains how AI differs from other technologies and presents the existing empirical research on the determinants of human “trust” in AI, conducted in multiple disciplines over the last 20 years. Based on the reviewed literature, we identify the form of AI representation (robot, virtual, and embedded) and its level of machine intelligence (i.e., its capabilities) as important antecedents to the development of trust and propose a framework that addresses the elements that shape users’ cognitive and emotional trust. Our review reveals the important role of AI’s tangibility, transparency, reliability, and immediacy behaviors in developing cognitive trust, and the role of AI’s anthropomorphism specifically for emotional trust. We also note several limitations in the current evidence base, such as the diversity of trust measures and overreliance on short-term, small sample, and experimental studies, where the development of trust is likely to be different than in longer-term, higher stakes field environments. Based on our review, we suggest the most promising paths for future research.},
  author = {Ella Glikson and Anita W. Woolley},
  title = {Human Trust in Artificial Intelligence: Review of Empirical Research},
  journal = {Academy of Management Annals},
  year = {2020},
  volume = {14},
  number = {2},
  pages = {627--660},
  publisher = {Academy of Management},
  doi = {10.5465/annals.2018.0057},
  url = {https://doi.org/10.5465/annals.2018.0057},
  keywords = {type: review, trust, artificial intelligence, empirical research}
  series = {AI Trust Analysis}
}

@article{Gerlich2023PerceptionsAI,
  abstract = {In this comprehensive study, insights from 1389 scholars across the US, UK, Germany, and Switzerland shed light on the multifaceted perceptions of artificial intelligence (AI). AI’s burgeoning integration into everyday life promises enhanced efficiency and innovation. The Trustworthy AI principles by the European Commission, emphasising data safeguarding, security, and judicious governance, serve as the linchpin for AI’s widespread acceptance. A correlation emerged between societal interpretations of AI’s impact and elements like trustworthiness, associated risks, and usage/acceptance. Those discerning AI’s threats often view its prospective outcomes pessimistically, while proponents recognise its transformative potential. These inclinations resonate with trust and AI’s perceived singularity. Consequently, factors such as trust, application breadth, and perceived vulnerabilities shape public consensus, depicting AI as humanity’s boon or bane. The study also accentuates the public’s divergent views on AI’s evolution, underlining the malleability of opinions amidst polarising narratives.},
  author = {Michael Gerlich},
  title = {Perceptions and Acceptance of Artificial Intelligence: A Multi-Dimensional Study},
  journal = {Social Sciences},
  year = {2023},
  volume = {12},
  number = {9},
  pages = {502},
  publisher = {MDPI},
  doi = {10.3390/socsci12090502},
  url = {https://doi.org/10.3390/socsci12090502},
  keywords = {type: empirical study, AI perception, user acceptance, multidimensional study}
  series = {Public Perception AI}
}

@article{Afroogh2024TrustAI,
  abstract = {The increasing use of artificial intelligence (AI) systems in our daily lives through various applications, services, and products highlights the significance of trust and distrust in AI from a user perspective. AI-driven systems have significantly diffused into various aspects of our lives, serving as beneficial “tools” used by human agents. These systems are also evolving to act as co-assistants or semi-agents in specific domains, potentially influencing human thought, decision-making, and agency. Trust and distrust in AI serve as regulators and could significantly control the level of this diffusion, as trust can increase, and distrust may reduce the rate of adoption of AI. Recently, a variety of studies focused on the different dimensions of trust and distrust in AI and its relevant considerations. In this systematic literature review, after conceptualizing trust in the current AI literature, we will investigate trust in different types of human–machine interaction and its impact on technology acceptance in different domains. Additionally, we propose a taxonomy of technical (i.e., safety, accuracy, robustness) and non-technical axiological (i.e., ethical, legal, and mixed) trustworthiness metrics, along with some trustworthy measurements. Moreover, we examine major trust-breakers in AI (e.g., autonomy and dignity threats) and trustmakers; and propose some future directions and probable solutions for the transition to a trustworthy AI.},
  author = {Saleh Afroogh and Ali Akbari and Emmie Malone and Mohammadali Kargar and Hananeh Alambeigi},
  title = {Trust in AI: Progress, Challenges, and Future Directions},
  journal = {Humanities and Social Sciences Communications},
  year = {2024},
  volume = {11},
  number = {1},
  pages = {1--15},
  publisher = {Springer Nature},
  doi = {10.1057/s41599-024-04044-8},
  url = {https://doi.org/10.1057/s41599-024-04044-8},
  keywords = {type: review, trust in AI, challenges, future directions}
  series = {AI Trust Futures}
}

@article{Schmid2024UserStudy,
  abstract = {Choosing a career and educational path is a challenging decision for young people. Career planning conversational agents (CAs) can assist by identifying suitable occupations and educational paths. Trustworthiness is an important dimension for the acceptance of a career planning CA and is influenced by several factors. We conducted a user study with n=114 participants across three schools in Germany to explore the trustworthiness of different career planning CAs. We examined the correlation between trustworthiness and perceived competence, autonomy, and social relatedness from self-determination theory (SDT), as well as the explainability of interactions and several usability dimensions of the assistants. These dimensions included the ability to guide the conversation, onboarding quality, error tolerance, and information relevance. We tested three different variants of the career planning assistant: a form-based assistant, an intent-based CA, and a large language model (LLM)-based CA. The results showed that the LLM-based CA was on average significantly more trustworthy and was perceived as more explainable than the intent-based CA. Key trust factors included conversation flexibility, chatbot credibility, intent recognition, and maintenance of a secure conversation. Additionally, perceived autonomy was crucial for trust across all types of assistants and perceived relatedness for the two CAs. Our findings highlight key areas essential for developing trustworthy CAs.},
  author = {Thorsten Zylowski and Nathalia Sautchuk-Patrício and Wladimir Hettmann},
  title = {User Study on the Trustworthiness, Usability and Explainability of Intent-based and Large Language Model-based Career Planning Conversational Agents},
  journal = {Proceedings of the 2024 ACM Conference on Human Factors in Computing Systems},
  year = {2024},
  volume = {},
  number = {},
  pages = {1--10},
  publisher = {ACM},
  doi = {10.1145/3702163.3702409},
  url = {https://doi.org/10.1145/3702163.3702409},
  keywords = {type: user study, trustworthiness, usability, explainability, conversational agents}
  series = {Conversational Agent UX}
}

@article{Holzinger2020Explainability,
  abstract = {Artificial intelligence and algorithmic decision-making processes are increasingly criticized for their black-box nature. Explainable AI approaches to trace human-interpretable decision processes from algorithms have been explored. Yet, little is known about algorithmic explainability from a human factors’ perspective. From the perspective of user interpretability and understandability, this study examines the effect of explainability in AI on user trust and attitudes toward AI. It conceptualizes causability as an antecedent of explainability and as a key cue of an algorithm and examines them in relation to trust by testing how they affect user perceived performance of AI-driven services. The results show the dual roles of causability and explainability in terms of its underlying links to trust and subsequent user behaviors. Explanations of why certain news articles are recommended generate users trust whereas causability of to what extent they can understand the explanations affords users emotional confidence. Causability lends the justification for what and how should be explained as it determines the relative importance of the properties of explainability. The results have implications for the inclusion of causability and explanatory cues in AI systems, which help to increase trust and help users to assess the quality of explanations. Causable explainable AI will help people understand the decision-making process of AI algorithms by bringing transparency and accountability into AI systems.},
  author = {Donghee Shin},
  title = {The Effects of Explainability and Causability on Perception, Trust, and Acceptance of AI},
  journal = {International Journal of Human-Computer Studies},
  year = {2020},
  volume = {146},
  number = {},
  pages = {102551},
  publisher = {Elsevier},
  doi = {10.1016/j.ijhcs.2020.102551},
  url = {https://doi.org/10.1016/j.ijhcs.2020.102551},
  keywords = {type: experimental, explainability, causability, trust, AI acceptance}
  series = {Explainability Effects}
}
